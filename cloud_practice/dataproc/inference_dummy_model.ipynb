{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92ec97c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m venv pyspark_venv\n",
    "# !source pyspark_venv/bin/activate\n",
    "# !pip install --upgrade pip\n",
    "# !pip install venv-pack pyarrow==14.0.0 pandas==2.0.0 numpy==1.24.2 catboost==1.2.2\n",
    "# !venv-pack -o pyspark_venv.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd6fc1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !hdfs dfs -mkdir /user/ubuntu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d42ce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import array, pandas_udf, col\n",
    "from catboost import CatBoostClassifier\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e65cf134",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"python\"\n",
    "os.environ['PYSPARK_PYTHON'] = \"./environment/bin/python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca8a4166",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "    SparkConf().setMaster(\"yarn\").setAppName(\"inference_dummy_model\")\n",
    "        .set(\"spark.executor.memory\", \"2g\")\n",
    "        .set(\"spark.driver.memory\", \"4g\")\n",
    "        .set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        .set(\"spark.yarn.dist.archives\", \"pyspark_venv.tar.gz#environment\")\n",
    ")\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da5371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs mkdir /user/ubuntu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25d3cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_INPUT_DATA_PATH = \"s3a://automl-otus-practice/data.csv\"\n",
    "DEFAULT_OUTPUT_DATA_PATH = \"/user/ubuntu/outcome\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6227b01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'has_more_payment_types', 'has_sequential', 'has_installments', 'avg_payment_value',\n",
    "    'mean_days_purchase_to_approved', 'mean_days_approved_to_carrier',\n",
    "    'mean_days_limit_to_carrier', 'buy_has_work_day', 'itens', 'sum_price',\n",
    "    'sum_freight', 'sum_same_city', 'sum_same_state', 'mean_distance_km',\n",
    "    'mean_p_name_lenght', 'mean_p_photos_qty', 'mean_p_weight_g',\n",
    "    'mean_volume', 'mean_length_width_ratio'\n",
    "]\n",
    "features_array = \"features\"\n",
    "order_id = \"order_id\"\n",
    "real_score = \"score\"\n",
    "predicted_score = \"predicted_score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79f63a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = (\n",
    "    spark.read\n",
    "    .format('csv')\n",
    "    .options(header='true', inferSchema='true', delimiter=\",\")\n",
    "    .load(DEFAULT_INPUT_DATA_PATH)\n",
    ")\n",
    "sdf = sdf.dropna()\n",
    "sdf = sdf.withColumn(features_array, array(*features))\n",
    "\n",
    "model = CatBoostClassifier().load_model(\"model.cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a99c3954",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(IntegerType())\n",
    "def inference_udf(input_data):\n",
    "    objects = np.vstack(input_data.to_numpy().ravel())\n",
    "    \n",
    "    \n",
    "    predictions = pd.Series(model.predict(objects).ravel())\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8940f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn(predicted_score, inference_udf(col(features_array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f80b83d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+---------------+\n",
      "|            order_id|            features|score|predicted_score|\n",
      "+--------------------+--------------------+-----+---------------+\n",
      "|000229ec398224ef6...|[0.0, 0.0, 1.0, 2...|    5|              5|\n",
      "|00024acbcdf0a6daa...|[0.0, 0.0, 1.0, 2...|    4|              5|\n",
      "+--------------------+--------------------+-----+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.select([order_id, features_array, real_score, predicted_score]).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81a4c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.repartition(4)\n",
    "sdf.select(\n",
    "    [order_id, features_array, real_score, predicted_score]\n",
    ").write.mode(\"overwrite\").parquet(DEFAULT_OUTPUT_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e988084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x   - ubuntu hadoop          0 2023-11-06 09:33 /user/ubuntu/mlops_ol/outcome\r\n",
      "-rw-r--r--   1 ubuntu hadoop          0 2023-11-06 09:33 /user/ubuntu/mlops_ol/outcome/_SUCCESS\r\n",
      "-rw-r--r--   1 ubuntu hadoop     974112 2023-11-06 09:33 /user/ubuntu/mlops_ol/outcome/part-00000-fbf7fcb8-7b75-4a2c-a544-97f4c55c2e05-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 ubuntu hadoop     975681 2023-11-06 09:33 /user/ubuntu/mlops_ol/outcome/part-00001-fbf7fcb8-7b75-4a2c-a544-97f4c55c2e05-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 ubuntu hadoop     973214 2023-11-06 09:33 /user/ubuntu/mlops_ol/outcome/part-00002-fbf7fcb8-7b75-4a2c-a544-97f4c55c2e05-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 ubuntu hadoop     974091 2023-11-06 09:33 /user/ubuntu/mlops_ol/outcome/part-00003-fbf7fcb8-7b75-4a2c-a544-97f4c55c2e05-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -R /user/ubuntu/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45b61ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
